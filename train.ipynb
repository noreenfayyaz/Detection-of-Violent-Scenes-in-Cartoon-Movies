{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from keras.layers import Activation, Dense,Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "frame_sequence = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 8, 112, 112, 64)  1792      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 8, 56, 56, 64)    36928     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 8, 28, 28, 64)    0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 8, 14, 14, 128)   73856     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 8, 7, 7, 128)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 8, 6272)          0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 8, 512)            13895680  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 512)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 8, 256)            131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 256)            0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8, 5)              1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,140,869\n",
      "Trainable params: 14,140,869\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#input layer\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(64,(3,3),padding='same', strides=(2,2), activation='relu'),\n",
    "      input_shape=(frame_sequence, 224, 224, 3)\n",
    "    )\n",
    ")\n",
    "\n",
    "# first conv, 64\n",
    "model.add(\n",
    "    TimeDistributed( \n",
    "        Conv2D(64, (3,3), \n",
    "            padding='same', strides=(2,2), activation='relu')\n",
    "    )\n",
    ")\n",
    "\n",
    "#pooling\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        MaxPooling2D((2,2), strides=(2,2))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Second conv, 128\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(128, (3,3),\n",
    "            padding='same', strides=(2,2), activation='relu')\n",
    "    )\n",
    ")\n",
    "\n",
    " \n",
    "#pooling\n",
    "model.add(\n",
    "    TimeDistributed(\n",
    "        MaxPooling2D((2,2), strides=(2,2))\n",
    "    )\n",
    ")\n",
    "\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "\n",
    "model.add(\n",
    "    LSTM(512, activation='relu', return_sequences=True)\n",
    ")\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TimeDistributedImageDataGenerator import TimeDistributedImageDataGenerator\n",
    "datagen = TimeDistributedImageDataGenerator.TimeDistributedImageDataGenerator(time_steps = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6816 images belonging to 5 classes.\n",
      "Found 2267 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# prepare an iterators for each dataset\n",
    "train_it = datagen.flow_from_directory('Dataset/train_set/', class_mode='categorical' ,color_mode='rgb', target_size=(224, 224) , batch_size=batch_size)#set as training data\n",
    "val_it = datagen.flow_from_directory('Dataset/val_set/', class_mode='categorical' ,color_mode='rgb' , target_size=(224, 224) , batch_size=batch_size )#set as validation data\n",
    "#test_it = datagen.flow_from_directory('test_set/', class_mode='categorical' ,color_mode='rgb', target_size=(64, 64) , batch_size=batch_size, shuffle=False,     seed=42)#set as training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape=(32, 8, 224, 224, 3), min=0.000, max=255.000\n"
     ]
    }
   ],
   "source": [
    "#to find the batch and dim of the loading images\n",
    "batchX, batchy = train_it.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blast', 'blood', 'fight', 'gunshots', 'normal'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_it.class_indices.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\gpu1\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "opt = tensorflow.keras.optimizers.SGD(lr=0.0001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n",
    "#model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# define checkpoint callback\n",
    "filepath = 'saved_models/model-ep{epoch:02d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enter the weights of the model from where you start resuming incase the training is not going to be completed in one pass\n",
    "model.load_weights('saved_models/model-ep50-loss0.306-val_loss0.235.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\gpu1\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000269CC27F2F0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000269CC27F2F0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.3010 - accuracy: 0.8901WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000269D9462F28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x00000269D9462F28> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "Epoch 51: saving model to saved_models\\model-ep51-loss0.301-val_loss0.235.h5\n",
      "213/213 [==============================] - 498s 2s/step - loss: 0.3010 - accuracy: 0.8901 - val_loss: 0.2346 - val_accuracy: 0.9213\n",
      "Epoch 52/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2966 - accuracy: 0.8919\n",
      "Epoch 52: saving model to saved_models\\model-ep52-loss0.297-val_loss0.228.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2966 - accuracy: 0.8919 - val_loss: 0.2281 - val_accuracy: 0.9236\n",
      "Epoch 53/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2902 - accuracy: 0.8959\n",
      "Epoch 53: saving model to saved_models\\model-ep53-loss0.290-val_loss0.225.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2902 - accuracy: 0.8959 - val_loss: 0.2249 - val_accuracy: 0.9255\n",
      "Epoch 54/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.8976\n",
      "Epoch 54: saving model to saved_models\\model-ep54-loss0.284-val_loss0.222.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2838 - accuracy: 0.8976 - val_loss: 0.2221 - val_accuracy: 0.9258\n",
      "Epoch 55/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2836 - accuracy: 0.8976\n",
      "Epoch 55: saving model to saved_models\\model-ep55-loss0.284-val_loss0.223.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2836 - accuracy: 0.8976 - val_loss: 0.2233 - val_accuracy: 0.9267\n",
      "Epoch 56/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.9005\n",
      "Epoch 56: saving model to saved_models\\model-ep56-loss0.274-val_loss0.219.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2741 - accuracy: 0.9005 - val_loss: 0.2191 - val_accuracy: 0.9256\n",
      "Epoch 57/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2686 - accuracy: 0.9028\n",
      "Epoch 57: saving model to saved_models\\model-ep57-loss0.269-val_loss0.216.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2686 - accuracy: 0.9028 - val_loss: 0.2157 - val_accuracy: 0.9294\n",
      "Epoch 58/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2642 - accuracy: 0.9055\n",
      "Epoch 58: saving model to saved_models\\model-ep58-loss0.264-val_loss0.217.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2642 - accuracy: 0.9055 - val_loss: 0.2166 - val_accuracy: 0.9300\n",
      "Epoch 59/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9070\n",
      "Epoch 59: saving model to saved_models\\model-ep59-loss0.261-val_loss0.208.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.2608 - accuracy: 0.9070 - val_loss: 0.2082 - val_accuracy: 0.9315\n",
      "Epoch 60/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2552 - accuracy: 0.9083\n",
      "Epoch 60: saving model to saved_models\\model-ep60-loss0.255-val_loss0.208.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.2552 - accuracy: 0.9083 - val_loss: 0.2078 - val_accuracy: 0.9321\n",
      "Epoch 61/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2495 - accuracy: 0.9101\n",
      "Epoch 61: saving model to saved_models\\model-ep61-loss0.249-val_loss0.204.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.2495 - accuracy: 0.9101 - val_loss: 0.2035 - val_accuracy: 0.9325\n",
      "Epoch 62/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2481 - accuracy: 0.9107\n",
      "Epoch 62: saving model to saved_models\\model-ep62-loss0.248-val_loss0.206.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2481 - accuracy: 0.9107 - val_loss: 0.2055 - val_accuracy: 0.9321\n",
      "Epoch 63/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2419 - accuracy: 0.9135\n",
      "Epoch 63: saving model to saved_models\\model-ep63-loss0.242-val_loss0.200.h5\n",
      "213/213 [==============================] - 464s 2s/step - loss: 0.2419 - accuracy: 0.9135 - val_loss: 0.2004 - val_accuracy: 0.9338\n",
      "Epoch 64/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2357 - accuracy: 0.9164\n",
      "Epoch 64: saving model to saved_models\\model-ep64-loss0.236-val_loss0.203.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2357 - accuracy: 0.9164 - val_loss: 0.2032 - val_accuracy: 0.9325\n",
      "Epoch 65/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2342 - accuracy: 0.9149\n",
      "Epoch 65: saving model to saved_models\\model-ep65-loss0.234-val_loss0.195.h5\n",
      "213/213 [==============================] - 464s 2s/step - loss: 0.2342 - accuracy: 0.9149 - val_loss: 0.1950 - val_accuracy: 0.9367\n",
      "Epoch 66/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2266 - accuracy: 0.9181\n",
      "Epoch 66: saving model to saved_models\\model-ep66-loss0.227-val_loss0.195.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.2266 - accuracy: 0.9181 - val_loss: 0.1953 - val_accuracy: 0.9366\n",
      "Epoch 67/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2287 - accuracy: 0.9195\n",
      "Epoch 67: saving model to saved_models\\model-ep67-loss0.229-val_loss0.196.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2287 - accuracy: 0.9195 - val_loss: 0.1957 - val_accuracy: 0.9366\n",
      "Epoch 68/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2247 - accuracy: 0.9203\n",
      "Epoch 68: saving model to saved_models\\model-ep68-loss0.225-val_loss0.192.h5\n",
      "213/213 [==============================] - 459s 2s/step - loss: 0.2247 - accuracy: 0.9203 - val_loss: 0.1920 - val_accuracy: 0.9369\n",
      "Epoch 69/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9225\n",
      "Epoch 69: saving model to saved_models\\model-ep69-loss0.218-val_loss0.188.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2175 - accuracy: 0.9225 - val_loss: 0.1880 - val_accuracy: 0.9389\n",
      "Epoch 70/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9223\n",
      "Epoch 70: saving model to saved_models\\model-ep70-loss0.217-val_loss0.188.h5\n",
      "213/213 [==============================] - 460s 2s/step - loss: 0.2175 - accuracy: 0.9223 - val_loss: 0.1881 - val_accuracy: 0.9388\n",
      "Epoch 71/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9236\n",
      "Epoch 71: saving model to saved_models\\model-ep71-loss0.213-val_loss0.187.h5\n",
      "213/213 [==============================] - 460s 2s/step - loss: 0.2134 - accuracy: 0.9236 - val_loss: 0.1874 - val_accuracy: 0.9382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9248\n",
      "Epoch 72: saving model to saved_models\\model-ep72-loss0.210-val_loss0.186.h5\n",
      "213/213 [==============================] - 459s 2s/step - loss: 0.2102 - accuracy: 0.9248 - val_loss: 0.1864 - val_accuracy: 0.9388\n",
      "Epoch 73/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2082 - accuracy: 0.9251\n",
      "Epoch 73: saving model to saved_models\\model-ep73-loss0.208-val_loss0.184.h5\n",
      "213/213 [==============================] - 460s 2s/step - loss: 0.2082 - accuracy: 0.9251 - val_loss: 0.1836 - val_accuracy: 0.9416\n",
      "Epoch 74/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2041 - accuracy: 0.9274\n",
      "Epoch 74: saving model to saved_models\\model-ep74-loss0.204-val_loss0.184.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.2041 - accuracy: 0.9274 - val_loss: 0.1841 - val_accuracy: 0.9412\n",
      "Epoch 75/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2016 - accuracy: 0.9282 \n",
      "Epoch 75: saving model to saved_models\\model-ep75-loss0.202-val_loss0.182.h5\n",
      "213/213 [==============================] - 3082s 15s/step - loss: 0.2016 - accuracy: 0.9282 - val_loss: 0.1821 - val_accuracy: 0.9412\n",
      "Epoch 76/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.2002 - accuracy: 0.9289\n",
      "Epoch 76: saving model to saved_models\\model-ep76-loss0.200-val_loss0.183.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.2002 - accuracy: 0.9289 - val_loss: 0.1830 - val_accuracy: 0.9406\n",
      "Epoch 77/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9311\n",
      "Epoch 77: saving model to saved_models\\model-ep77-loss0.197-val_loss0.177.h5\n",
      "213/213 [==============================] - 464s 2s/step - loss: 0.1973 - accuracy: 0.9311 - val_loss: 0.1772 - val_accuracy: 0.9449\n",
      "Epoch 78/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9320\n",
      "Epoch 78: saving model to saved_models\\model-ep78-loss0.194-val_loss0.177.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.1942 - accuracy: 0.9320 - val_loss: 0.1768 - val_accuracy: 0.9433\n",
      "Epoch 79/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9335\n",
      "Epoch 79: saving model to saved_models\\model-ep79-loss0.190-val_loss0.179.h5\n",
      "213/213 [==============================] - 465s 2s/step - loss: 0.1896 - accuracy: 0.9335 - val_loss: 0.1785 - val_accuracy: 0.9416\n",
      "Epoch 80/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1856 - accuracy: 0.9352\n",
      "Epoch 80: saving model to saved_models\\model-ep80-loss0.186-val_loss0.176.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.1856 - accuracy: 0.9352 - val_loss: 0.1759 - val_accuracy: 0.9427\n",
      "Epoch 81/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1844 - accuracy: 0.9327\n",
      "Epoch 81: saving model to saved_models\\model-ep81-loss0.184-val_loss0.177.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.1844 - accuracy: 0.9327 - val_loss: 0.1770 - val_accuracy: 0.9423\n",
      "Epoch 82/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9357\n",
      "Epoch 82: saving model to saved_models\\model-ep82-loss0.181-val_loss0.174.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.1814 - accuracy: 0.9357 - val_loss: 0.1735 - val_accuracy: 0.9450\n",
      "Epoch 83/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9363\n",
      "Epoch 83: saving model to saved_models\\model-ep83-loss0.181-val_loss0.173.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.1807 - accuracy: 0.9363 - val_loss: 0.1731 - val_accuracy: 0.9452\n",
      "Epoch 84/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9365\n",
      "Epoch 84: saving model to saved_models\\model-ep84-loss0.179-val_loss0.170.h5\n",
      "213/213 [==============================] - 463s 2s/step - loss: 0.1793 - accuracy: 0.9365 - val_loss: 0.1703 - val_accuracy: 0.9460\n",
      "Epoch 85/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1775 - accuracy: 0.9372\n",
      "Epoch 85: saving model to saved_models\\model-ep85-loss0.178-val_loss0.169.h5\n",
      "213/213 [==============================] - 464s 2s/step - loss: 0.1775 - accuracy: 0.9372 - val_loss: 0.1689 - val_accuracy: 0.9464\n",
      "Epoch 86/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9390\n",
      "Epoch 86: saving model to saved_models\\model-ep86-loss0.174-val_loss0.170.h5\n",
      "213/213 [==============================] - 459s 2s/step - loss: 0.1741 - accuracy: 0.9390 - val_loss: 0.1700 - val_accuracy: 0.9449\n",
      "Epoch 87/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1713 - accuracy: 0.9393\n",
      "Epoch 87: saving model to saved_models\\model-ep87-loss0.171-val_loss0.169.h5\n",
      "213/213 [==============================] - 461s 2s/step - loss: 0.1713 - accuracy: 0.9393 - val_loss: 0.1689 - val_accuracy: 0.9461\n",
      "Epoch 88/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9409\n",
      "Epoch 88: saving model to saved_models\\model-ep88-loss0.170-val_loss0.166.h5\n",
      "213/213 [==============================] - 460s 2s/step - loss: 0.1705 - accuracy: 0.9409 - val_loss: 0.1663 - val_accuracy: 0.9471\n",
      "Epoch 89/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1648 - accuracy: 0.9415\n",
      "Epoch 89: saving model to saved_models\\model-ep89-loss0.165-val_loss0.169.h5\n",
      "213/213 [==============================] - 462s 2s/step - loss: 0.1648 - accuracy: 0.9415 - val_loss: 0.1685 - val_accuracy: 0.9461\n",
      "Epoch 90/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9426\n",
      "Epoch 90: saving model to saved_models\\model-ep90-loss0.163-val_loss0.167.h5\n",
      "213/213 [==============================] - 460s 2s/step - loss: 0.1634 - accuracy: 0.9426 - val_loss: 0.1667 - val_accuracy: 0.9461\n",
      "Epoch 91/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1653 - accuracy: 0.9421\n",
      "Epoch 91: saving model to saved_models\\model-ep91-loss0.165-val_loss0.163.h5\n",
      "213/213 [==============================] - 456s 2s/step - loss: 0.1653 - accuracy: 0.9421 - val_loss: 0.1633 - val_accuracy: 0.9477\n",
      "Epoch 92/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9418\n",
      "Epoch 92: saving model to saved_models\\model-ep92-loss0.164-val_loss0.165.h5\n",
      "213/213 [==============================] - 454s 2s/step - loss: 0.1637 - accuracy: 0.9418 - val_loss: 0.1653 - val_accuracy: 0.9477\n",
      "Epoch 93/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1568 - accuracy: 0.9456\n",
      "Epoch 93: saving model to saved_models\\model-ep93-loss0.157-val_loss0.167.h5\n",
      "213/213 [==============================] - 456s 2s/step - loss: 0.1568 - accuracy: 0.9456 - val_loss: 0.1668 - val_accuracy: 0.9456\n",
      "Epoch 94/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9442\n",
      "Epoch 94: saving model to saved_models\\model-ep94-loss0.157-val_loss0.164.h5\n",
      "213/213 [==============================] - 453s 2s/step - loss: 0.1570 - accuracy: 0.9442 - val_loss: 0.1643 - val_accuracy: 0.9479\n",
      "Epoch 95/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1525 - accuracy: 0.9463\n",
      "Epoch 95: saving model to saved_models\\model-ep95-loss0.152-val_loss0.161.h5\n",
      "213/213 [==============================] - 454s 2s/step - loss: 0.1525 - accuracy: 0.9463 - val_loss: 0.1608 - val_accuracy: 0.9488\n",
      "Epoch 96/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9463\n",
      "Epoch 96: saving model to saved_models\\model-ep96-loss0.153-val_loss0.161.h5\n",
      "213/213 [==============================] - 452s 2s/step - loss: 0.1531 - accuracy: 0.9463 - val_loss: 0.1613 - val_accuracy: 0.9493\n",
      "Epoch 97/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1546 - accuracy: 0.9461\n",
      "Epoch 97: saving model to saved_models\\model-ep97-loss0.155-val_loss0.162.h5\n",
      "213/213 [==============================] - 454s 2s/step - loss: 0.1546 - accuracy: 0.9461 - val_loss: 0.1621 - val_accuracy: 0.9488\n",
      "Epoch 98/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9477\n",
      "Epoch 98: saving model to saved_models\\model-ep98-loss0.149-val_loss0.162.h5\n",
      "213/213 [==============================] - 450s 2s/step - loss: 0.1487 - accuracy: 0.9477 - val_loss: 0.1621 - val_accuracy: 0.9496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1484 - accuracy: 0.9482\n",
      "Epoch 99: saving model to saved_models\\model-ep99-loss0.148-val_loss0.161.h5\n",
      "213/213 [==============================] - 450s 2s/step - loss: 0.1484 - accuracy: 0.9482 - val_loss: 0.1606 - val_accuracy: 0.9495\n",
      "Epoch 100/100\n",
      "213/213 [==============================] - ETA: 0s - loss: 0.1466 - accuracy: 0.9480\n",
      "Epoch 100: saving model to saved_models\\model-ep100-loss0.147-val_loss0.159.h5\n",
      "213/213 [==============================] - 451s 2s/step - loss: 0.1466 - accuracy: 0.9480 - val_loss: 0.1585 - val_accuracy: 0.9509\n",
      "25662.487069100003\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "history=model.fit_generator(\n",
    "        train_it,\n",
    "        steps_per_epoch=None,# nb_training_samples // batch_size,\n",
    "        initial_epoch=50,     #from where you start resuming epochs\n",
    "        epochs=100,        # the number of epochs to which it runs\n",
    "        validation_data=val_it,\n",
    "        validation_steps=None,         # nb_validation_samples // batch_size,\n",
    "        callbacks=[tensorboard_callback,checkpoint],\n",
    "        verbose = 1)\n",
    "\n",
    "print(time.perf_counter()-start_time)\n",
    "\n",
    "\n",
    "# save model and architecture to single file\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"best_model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"best_model1.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "opt = tensorflow.keras.optimizers.SGD(lr=0.001)\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val_test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'Val_test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "model.evaluate_generator(generator=val_it,\n",
    "steps=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the output \n",
    "\n",
    "test_it.reset()\n",
    "pred=model.predict_generator(test_it,\n",
    "steps=324,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
